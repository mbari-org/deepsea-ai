{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " <div align=\"center\">\n",
    "\n",
    "  <a href=\"https://www.mbari.org/\" target=\"_blank\">\n",
    "    <img width=\"512\", src=\"https://www.mbari.org/wp-content/uploads/2014/11/logo-mbari-3b.png\"></a>\n",
    "\n",
    "\n",
    "This notebook by <a href=\"https://mbari.org\">MBARI</a> demonstrates how to train a YOLOv5 object detection model using freely available FathomNet data using AWS. See the <a href=\"https://docs.mbari.org/deepsea-ai/\">full documentation</a> on how to setup the deepsea-ai module or check <a href=\"https://github.com/mbari-org/deepsea-ai/discussions\">github</a> for questions.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "If you have successfully setup your Anaconda environment, you should see the kernel **deepsea-ai-notebooks**.\n",
    "Select that kernel.\n",
    "\n",
    "A few housekeeping items:\n",
    "\n",
    "- The packages installed require python3.7 or greater, so let's check the python version first, then install the additional packages used in this notebook\n",
    "- Verify your AWS account is working by listing your buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U -q fathomnet pillow\n",
    "!pip install -U -q deepsea-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select concepts\n",
    "\n",
    "Let's choose some concepts that are in the FathomNet database and put them in a list.  We are also going to put them into a file, one line per concept in alphabetical order which will be used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "concepts = [\"Rathbunaster californicus\", \"Holothuroidea\", \"Strongylocentrotus fragilis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('mynames.txt', 'w+') as n:\n",
    "  for c in sorted(concepts):\n",
    "    n.write(c + '\\r\\n')\n",
    "\n",
    "#This should be in alphabetical order\n",
    "!cat mynames.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the `fathomnet.api.images.find_by_concept` function to query how many images are available for these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fathomnet.api import images\n",
    "\n",
    "for c in concepts:\n",
    "  available_images = images.find_by_concept(c)\n",
    "  print(f'{c} images: {len(available_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the images and format the labels \n",
    "\n",
    "This code will:\n",
    "* **Download** the images and bounding box annotations from FathomNet and save them to the ```trainingdata/``` folder.\n",
    "* **Format** the bounding boxes into a format compatible with the object detection model.  The YOLOv5 model uses a very simple format which is a text file, one line per annotation, e.g. ```\n",
    "0 101 33 83 199\n",
    "``` which corresponds to```<label index 0-based> <x> <y> <width> <height>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from progressbar import progressbar\n",
    "from fathomnet.api import images\n",
    "from PIL import Image\n",
    "from fathomnet.models import GeoImageConstraints\n",
    "\n",
    "# Create a directory for the images and labels\n",
    "data_dir = Path.cwd() / 'trainingdata'\n",
    "\n",
    "image_dir = data_dir / 'images'\n",
    "image_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "label_dir = data_dir / 'labels'\n",
    "label_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "# Download each image and create a label file for training\n",
    "image_paths = []\n",
    "\n",
    "for c in concepts:\n",
    "  \n",
    "  # Constrain to only 50 images\n",
    "  concept_constrained = GeoImageConstraints(concept=c, limit=50)\n",
    "  concept_images = images.find(concept_constrained)\n",
    "\n",
    "  print(f'Downloading {c} images: {len(concept_images)}')\n",
    "    \n",
    "  for image in progressbar(concept_images):\n",
    "    # Export labels with the same name as the unique identifier appended with .txt per darknet format\n",
    "    label = label_dir / f'{image.uuid}.txt'\n",
    "    print(image.boundingBoxes)\n",
    "\n",
    "    # Export to darknet format, which is one line per annotation <label index 1-based> <x> <y> <width> <height>\n",
    "    with label.open(\"w+\") as l:\n",
    "      for b in image.boundingBoxes:\n",
    "        # Only save concepts in our list\n",
    "        if b.concept in concepts:\n",
    "          l.write(f'{concepts.index(b.concept)} {b.x/image.width} {b.y/image.height} {b.width/image.width} {b.height/image.height}\\n')\n",
    "    \n",
    "    # # Format our image file name as the image UUID + .jpg\n",
    "    image_path = image_dir / f'{image.uuid}.jpg'\n",
    "    image_paths.append(image_path)\n",
    "    if image_path.exists():  # Skip re-downloading images\n",
    "      continue\n",
    "    \n",
    "    # # Download the image\n",
    "    image_raw = requests.get(image.url, stream=True).raw\n",
    "    pil_image = Image.open(image_raw)\n",
    "    \n",
    "    # Convert to RGB (ensures consistent colorspace)\n",
    "    pil_image = pil_image.convert('RGB')\n",
    "\n",
    "    # Save the image\n",
    "    pil_image.save(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Now that we have some training data, let's train it for a few cycles.\n",
    "\n",
    "Before we train it, we need to split the data into training, testing with the **deepsea-ai split** command.  This will split the data, then compressed it into images.tar.gz and labels.tar.gz files to reduce the data size.\n",
    "\n",
    "\n",
    "The training data downloaded should be organized something like:\n",
    "\n",
    "```\n",
    "├── trainingdata\n",
    "│   │   ├── images\n",
    "│   │   │   └── image1.jpg\n",
    "│   │   │   └── image2.jpg\n",
    "│   │   ├── labels\n",
    "│   │   │   └── image1.txt\n",
    "│   │   │   └── image2.txt \n",
    "```\n",
    "\n",
    "and after running the split command, you should have just two files\n",
    "\n",
    "```\n",
    "├── splitdata\n",
    "│   │   ├── images.tar.gz\n",
    "│   │   ├── labels.tar.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!deepsea-ai split -i trainingdata -o splitdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the object detection model for a few epochs\n",
    "\n",
    "The object detector model is the highly performant [YOLOv5 model](http://github.com/ultralytics/yolov5). This represents a good tradeoff between cost and performance. See [training](https://docs.mbari.org/deepsea-ai/commands/train/) for the full documentation.\n",
    "\n",
    "To track cost, we will setup unique tags for our project, and pass that to the ``--config`` option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile deepseailab.txt\n",
    "[docker]\n",
    "yolov5_container = mbari/deepsea-yolov5:1.1.2\n",
    "strongsort_container = mbari/strongsort-yolov5:5c05df0\n",
    "\n",
    "[aws]\n",
    "account_id = 548531997526\n",
    "sagemaker_arn = arn:aws:iam::548531997526:role/DeepSeaAI\n",
    "model = s3://deepsea-ai-548531997526-models/yolov5x_mbay_benthic_model.tar.gz\n",
    "track_config = s3://deepsea-ai-548531997526-track-conf/strong_sort_benthic.yaml\n",
    "videos = s3://deepsea-ai-548531997526-videos\n",
    "models = s3://deepsea-ai-548531997526-models\n",
    "tracks = s3://deepsea-ai-548531997526-tracks\n",
    "\n",
    "[aws_public]\n",
    "model = s3://902005-public/models/yolov5x_mbay_benthic_model.tar.gz\n",
    "track_config = s3://902005-public/models/track-config/strong_sort_benthic.yaml\n",
    "video_ex1 = s3://902005-public/video/1sec/V4361_20211006T162656Z_h265_1sec.mp4\n",
    "video_ex2 = s3://902005-public/video/1sec/V4361_20211006T163256Z_h265_1sec.mp4\n",
    "video_ex3 = s3://902005-public/video/1sec/V4361_20211006T163856Z_h265_1sec.mp4\n",
    " \n",
    "[database]\n",
    "site = http://localhost:4000\n",
    "gql = %(site)/graphql\n",
    "\n",
    "[tags]\n",
    "organization = deepseailab\n",
    "project_number = NA\n",
    "stage = test\n",
    "application = detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "## Create a unique bucket name\n",
    "\n",
    "Please note that bucket names need to be globally unique.  Let's set the bucket name here with a randomly generated string to help with that. You can remove this and replace with your unique bucket name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "uuid_short = str(uuid.uuid4())[:8]\n",
    "uuid_short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "We set the number of epochs to **2** which is far too small to be useful, but we will test it first to make sure your AWS account works ok.  We will use the ml.p3.2xlarge instance which is at the time of this writing $3.06 USD per hour.  Once this is done, you might try it again for a longer period to see if the performance improves, or try a larger instance type, or try downloading more images.  At the time of this writing, typically less than 40 epochs are needed; beyond that performance does not improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!deepsea-ai train --model yolov5x --instance-type ml.p3.2xlarge \\\n",
    "--labels splitdata/labels.tar.gz \\\n",
    "--images splitdata/images.tar.gz \\\n",
    "--label-map mynames.txt \\\n",
    "--config deepseailab.txt \\\n",
    "--input-s3 f's3://{uuid_short}-deepseailab-benthic-training/' \\\n",
    "--output-s3 f's3://{uuid_short}-deepseailab-benthic-checkpoints/' \\\n",
    "--epochs 2 \\\n",
    "--batch-size 2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deepsea-ai-notebooks",
   "language": "python",
   "name": "deepsea-ai-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
